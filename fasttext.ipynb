{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP - You need these only the first time you run the code\n",
    "#!git clone https://github.com/facebookresearch/fastText.git\n",
    "#cd fastText\n",
    "#!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ft_helpers as fth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module fasttext.FastText in fasttext:\n",
      "\n",
      "NAME\n",
      "    fasttext.FastText\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright (c) 2017-present, Facebook, Inc.\n",
      "    # All rights reserved.\n",
      "    #\n",
      "    # This source code is licensed under the MIT license found in the\n",
      "    # LICENSE file in the root directory of this source tree.\n",
      "\n",
      "FUNCTIONS\n",
      "    cbow(*kargs, **kwargs)\n",
      "    \n",
      "    eprint(*args, **kwargs)\n",
      "    \n",
      "    load_model(path)\n",
      "        Load a model given a filepath and return a model object.\n",
      "    \n",
      "    read_args(arg_list, arg_dict, arg_names, default_values)\n",
      "    \n",
      "    skipgram(*kargs, **kwargs)\n",
      "    \n",
      "    supervised(*kargs, **kwargs)\n",
      "    \n",
      "    tokenize(text)\n",
      "        Given a string of text, tokenize it and return a list of tokens\n",
      "    \n",
      "    train_supervised(*kargs, **kwargs)\n",
      "        Train a supervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input file must must contain at least one label per line. For an\n",
      "        example consult the example datasets which are part of the fastText\n",
      "        repository such as the dataset pulled by classification-example.sh.\n",
      "    \n",
      "    train_unsupervised(*kargs, **kwargs)\n",
      "        Train an unsupervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input field must not contain any labels or use the specified label prefix\n",
      "        unless it is ok for those words to be ignored. For an example consult the\n",
      "        dataset pulled by the example script word-vector-example.sh, which is\n",
      "        part of the fastText repository.\n",
      "\n",
      "DATA\n",
      "    BOW = '<'\n",
      "    EOS = '</s>'\n",
      "    EOW = '>'\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    displayed_errors = {}\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "    unsupervised_default = {'autotuneDuration': 300, 'autotuneMetric': 'f1...\n",
      "\n",
      "FILE\n",
      "    /opt/anaconda3/lib/python3.7/site-packages/fasttext/FastText.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fasttext.FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "tweets, test = fth.load_data(full=True)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "train, val = fth.train_val_split(tweets['body'], tweets['label'], 0.3, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex the dataframe for fasttext format\n",
    "train = fth.reindex_df(train)\n",
    "val = fth.reindex_df(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes into .txt for fasttext\n",
    "np.savetxt(r'train_full.txt', train.values, fmt='%s')\n",
    "np.savetxt(r'val_full.txt', val.values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train fasttext model on training set\n",
    "model = fasttext.train_supervised(input=\"train_full.txt\", lr=1.0, epoch=1, wordNgrams=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750000, 0.8638973333333333, 0.8638973333333333)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model on validation set\n",
    "model.test(\"val_full.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### w/o hyperparameter tuning:\n",
    "- dataset=small: (60000, 0.82505, 0.82505)\n",
    "- dataset=full: (750000, 0.8355986666666667, 0.8355986666666667)\n",
    "\n",
    "\n",
    "### w/ some hyperparameter tuning:\n",
    "- dataset=small, lr=1.0, epoch=1, wordNgrams=3: (60000, 0.8371166666666666, 0.8371166666666666)\n",
    "- dataset=full, lr=1.0, epoch=1, wordNgrams=3: (750000, 0.8639346666666666, 0.8639346666666666)\n",
    "\n",
    "- dataset=full, lr=0.3, epoch=1, wordNgrams=3: (750000, 0.86366, 0.86366)\n",
    "- dataset=full, lr=0.3, epoch=20, wordNgrams=3: (750000, 0.847332, 0.847332)\n",
    "\n",
    "- dataset=full, lr=1.0, epoch=1, wordNgrams=4: (750000, 0.86412, 0.86412) \n",
    "- dataset=full, lr=1.0, epoch=1, wordNgrams=5: (750000, 0.8628946666666667, 0.8628946666666667)\n",
    "- dataset=full, lr=1.0, epoch=1, wordNgrams=6: (750000, 0.8615613333333333, 0.8615613333333333)\n",
    "- dataset=full, lr=1.0, epoch=1, wordNgrams=7: (750000, 0.860692, 0.860692)\n",
    "- dataset=full, lr=1.0, epoch=1, wordNgrams=8: (750000, 0.859468, 0.859468)\n",
    "\n",
    "- dataset=full, lr=1.0, epoch=100, wordNgrams=4: 750000, 0.8457546666666667, 0.8457546666666667)\n",
    "\n",
    "- dataset=full, lr=1.0, epoch=1: (750000, 0.8324173333333333, 0.8324173333333333)\n",
    "- dataset=full, lr=1.0, epoch=10: (750000, 0.792976, 0.792976)\n",
    "\n",
    "Best parameters: lr=1.0, epoch=1, ngrams=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To continue, apply the methods suggested in this tutorial: https://fasttext.cc/docs/en/supervised-tutorial.html\n",
    "- More FastText documentation here: https://fasttext.cc\n",
    "\n",
    "### TODO: \n",
    "1. Try more ngrams and more hyperparameters --done \n",
    "2. Comment more -- done\n",
    "3. Create more functions (make the code more modular) --> e.g. another function for train test splitting alone -- done\n",
    "3. Try with the preprocessed data \n",
    "4. Create submission csv and submit to aicrowd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
